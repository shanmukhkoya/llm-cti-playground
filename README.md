# üí¨ llm-cti-playground

An interactive, modular GenAI playground to explore Large Language Models (LLMs) hands-on ‚Äî tailored for Contact Center Integration (CTI) solution architects. This project is structured into independent **phases**, each teaching core GenAI concepts with minimal setup, clear code, and lightweight dependencies.

---

## üß† Model Choice & Setup

### üöÄ Why TinyLLaMA Is Our Priority

- ‚úÖ **Lightweight & Fast**: Just ~637MB, perfect for WSL or cloud PCs.
- ‚úÖ **Low Resource**: Runs without GPU, internet, or external dependencies.
- ‚úÖ **Learning-Friendly**: Ideal for rapid prototyping & understanding LLM internals.
- ‚úÖ **Ollama-Compatible**: Swap to `llama3`, `mistral`, `phi3` easily later.
- ‚úÖ **Scalable**: Upgrade-ready without changing your app code.

---

### üîß Setting Up Ollama & Model

1. **Install Ollama**  
   ‚Üí Follow [https://ollama.com/download](https://ollama.com/download)

2. **Pull & Run TinyLLaMA**

   ```bash
   ollama pull tinyllama
   ollama run tinyllama
   ```

3. **Switch to Another Model (Optional)**

   ```bash
   ollama pull mistral
   ollama run mistral
   ```

---

## üîç Project Phases Overview

Each phase is isolated ‚Äî run and understand them independently or combine them progressively.

---

### üìü Phase 1: CLI Chat with LLM

- **Folder**: `phase1_llm_core`
- **Goal**: Text-based interaction with TinyLLaMA in terminal.

```bash
python chat_cli.py
```

- Sends prompt ‚Üí receives response using Ollama's local LLM API.
- Great to understand basic prompt-response flow.

---

### üåê Phase 2: Streamlit Chat UI

- **Folder**: `phase2_chat_ui`
- **Goal**: Simple chat web app using Streamlit.

```bash
streamlit run app.py
```

- Replaces terminal with an elegant browser-based UI.
- Keeps backend same (TinyLLaMA via Ollama).

---

### üß† Phase 3: RAG for Agent Assist

- **Folder**: `phase3_agent_assist`
- **Goal**: Build Retrieval-Augmented Generation (RAG) engine to power CTI Agent Assist use cases.

#### üìù Ingest Docs into Vector DB

```bash
python ingest.py
```

#### üîç Query LLM with Retrieved Context

```bash
python rag_chain.py
```

- **Supported Files**: `.pdf`, `.docx`, `.txt`
- Uses `ChromaDB` + `Sentence Transformers`
- Retrieves chunks relevant to query before calling the LLM.

---

### üí° Phase 4: LiteMind Chat (RAG + Streamlit UI)

- **Folder**: `phase4_litemind_chat`
- **Goal**: Unified, production-style app combining RAG + UI.

```bash
streamlit run app.py
```

- üìÅ Uses previously ingested documents.
- üß† Maintains **chat history** (session memory).
- üîÑ Shows **"thinking..."** while model responds.
- üí¨ Feels like a mini ChatGPT trained on your internal docs.

![Phase 4 UI](images/phase4_ui.png)

---

## üìÇ Project Structure

```
llm-cti-playground/
‚îú‚îÄ‚îÄ phase1_llm_core/         # CLI interaction
‚îú‚îÄ‚îÄ phase2_chat_ui/          # Streamlit chat UI
‚îú‚îÄ‚îÄ phase3_agent_assist/     # RAG document pipeline
‚îú‚îÄ‚îÄ phase4_litemind_chat/    # Streamlit + RAG combo
‚îú‚îÄ‚îÄ chroma_store/            # Chroma vector DB (ignored in Git)
‚îú‚îÄ‚îÄ venv/                    # Virtual environment
‚îú‚îÄ‚îÄ images/                  # Screenshots
‚îú‚îÄ‚îÄ requirements.txt
‚îú‚îÄ‚îÄ README.md
‚îú‚îÄ‚îÄ vision.md
‚îî‚îÄ‚îÄ .gitignore
```

---

## ‚öôÔ∏è Setup Instructions (Once)

```bash
# 1. Create virtual environment
python3 -m venv venv
source venv/bin/activate

# 2. Install dependencies
pip install -r requirements.txt

# 3. Ensure Ollama is running TinyLLaMA
ollama run tinyllama
```

---

## üß† Extra Notes

- üßæ **Document Support**: PDF, DOCX, TXT (others can be added)
- üíæ **Persistence**: Vector store in `chroma_store/` (excluded from Git)
- üß† **LLM Behavior**: Queries can fallback to model's pretraining if no relevant context is retrieved
- üì¶ **Python Version**: Recommended 3.10+

---

## ü§ù Contribute or Extend

- Add more phases like:
  - üì¢ **STT / TTS Voice Interface**
  - üó£Ô∏è **LangChain Agent Routing**
  - üì° **LLM API Gateway for Production**
- Replace `TinyLLaMA` with `phi3`, `llama3`, etc.

---

> Made with ‚ù§Ô∏è to help CTI Solution Architects embrace GenAI one phase at a time.
