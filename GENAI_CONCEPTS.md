# 🧠 GENAI_CONCEPTS.md

A comprehensive readout of Generative AI (GenAI) concepts and how they're applied in the `llm-cti-playground` project. This serves as a learning checkpoint and a reference guide for understanding *why* we used specific tools and *how* they relate to foundational GenAI concepts.

---

## 📘 1. What Is Generative AI?

Generative AI refers to systems that can generate human-like content (text, audio, images, code, etc.) using patterns learned from large datasets. It goes beyond just understanding input—it produces new, contextually relevant output.

---

## 🏗️ 2. Key Concepts and Their Analogies

| Concept                  | Analogy                                          | Explanation                                                                 |
|--------------------------|--------------------------------------------------|-----------------------------------------------------------------------------|
| Training Data            | Books a student studies                          | Large datasets (text/audio/images) that LLMs learn from.                    |
| Parameters               | What the student remembers                       | Tunable weights in the model learned during training (~billions).          |
| Embeddings               | Meaningful coordinates on a map                  | Converts words into numeric vectors capturing meaning.                     |
| Tokenization             | Breaking a sentence into LEGO blocks             | Text is split into tokens (subwords, words, characters).                   |
| Prompt                   | A question to the AI                             | Input query or instruction to the model.                                   |
| Response                 | AI’s answer to the question                      | Output generated by the model based on prompt + internal knowledge.        |
| Context Window           | Memory span during conversation                  | Maximum number of tokens the model can process at once.                    |
| RAG (Retrieval-Augmented Generation) | Open-book test                        | Retrieve relevant external documents before answering.                     |

---

## ⚙️ 3. Project-Wide GenAI Concepts and Mapping

### 🔹 3.1 Prompt Engineering

> **Prompt = Instruction + Context**

**Used In**:  
- `chat_cli.py` (CLI interaction)  
- `rag_chain.py` (context + question prompts)  
- `speech_chat.py` (voice to prompt)  

🔧 *We experimented with how the model responds differently based on prompt clarity, instruction style, and structure.*

---

### 🔹 3.2 Language Models (LLMs)

> Core engines trained on massive text corpora to predict the next word/token.

**We Used**:  
- `TinyLLaMA` via Ollama  
- Plug-n-play ready for `phi3`, `llama3`, `mistral`  

🔧 *Lightweight model choice (TinyLLaMA) was optimal for WSL/CPU-only setup.*

---

### 🔹 3.3 Tokenization

> Behind the scenes, every word gets converted to tokens before processing.

**Library Involved**:
- Internally handled by Ollama + model backend

🔧 *Important when understanding input limits and context sizes.*

---

### 🔹 3.4 Embeddings

> Embeddings = meaning in vector form  
Used to compare text similarity and find relevant information.

**We Used**:  
- `sentence-transformers` in `ingest.py`  
- Stored in `ChromaDB` (local vector store)  

🔧 *Embeddings power the document search before querying LLM in RAG.*

---

### 🔹 3.5 Vector Databases

> Think of them as search engines for meaning.

**We Used**:  
- `ChromaDB`  
- Stored in `chroma_store/` (not pushed to Git)

🔧 *Fast similarity search based on vector math to support RAG.*

---

### 🔹 3.6 Retrieval-Augmented Generation (RAG)

> Combines LLM with external knowledge.

**We Built**:  
- `rag_chain.py`: Retrieves → Constructs prompt → Sends to LLM  
- Uses PDF, DOCX, TXT documents  

🔧 *Mimics Agent Assist and enterprise chatbot architectures.*

---

### 🔹 3.7 Multi-modal Interfaces

> Different input/output types (text, audio, UI).

We implemented:
- 🎙️ **Speech-to-Text (STT)**: `speech_chat.py`, `stt_transcribe.py`
- 💬 **UI Chat**: Streamlit in `phase4_litemind_chat`
- 📂 **Docs as knowledge**: Agent Assist phase

---

## 🎙️ 4. Speech AI Concepts

### 🔹 4.1 Speech-to-Text (STT)

> Converts spoken words into text.

**Used**:
- `faster-whisper`: Lightweight, offline transcription
- `pydub`: Audio format conversion
- `speechrecognition`: Fallback or interface for mic input

🔧 *Perfectly adapted to work offline and with limited compute.*

---

### 🔹 4.2 Microphone Input (Optional)

> Live voice transcription from mic input.

**Tool**: `speechrecognition + PyAudio`  
💡 *Not supported inside WSL. We only used file-based STT due to environment limitations.*

---

## 🧪 5. Libraries & Their Purpose (By Phase)

| Phase | Libraries Used                                                                 |
|-------|---------------------------------------------------------------------------------|
| 1     | `requests`, `ollama`                                                           |
| 2     | `streamlit`, `requests`                                                        |
| 3     | `chromadb`, `sentence-transformers`, `PyPDF2`, `docx2txt`, `unstructured`      |
| 4     | All above + `streamlit`                                                        |
| 5     | `pydub`, `faster-whisper`, `speechrecognition`, `ffmpeg`                       |

> **FFmpeg** was essential for `.mp3` → `.wav` conversion.

---

## 🧠 6. LLM Application Types We Explored

| Use Case            | Example File / Phase                     |
|---------------------|------------------------------------------|
| CLI Chatbot         | `phase1_llm_core/chat_cli.py`            |
| Web Chat UI         | `phase2_chat_ui/app.py`                  |
| Agent Assist RAG    | `phase3_agent_assist`                    |
| ChatGPT-like UI     | `phase4_litemind_chat/app.py`            |
| Speech-to-LLM       | `phase5_speech_io/speech_chat.py`        |

---

## 🔮 7. Where This Is Leading

- ✅ Voice-to-LLM pipeline built (STT complete)
- 🔜 Next: Add **TTS (Text-to-Speech)** response
- 🔜 Later: Full **Voice Assistant Loop** (STT → LLM → TTS)

---

## 📌 Summary

This hands-on journey covered:

✅ Understanding LLM internals  
✅ Prompt engineering and conversational flow  
✅ Using embeddings + vector DBs for RAG  
✅ Web & terminal UIs for interaction  
✅ Speech input to LLM pipelines  
✅ Lightweight, portable design with $0 infra

---

> Made with ❤️ to enable CTI solution architects master GenAI — step by step.
-
